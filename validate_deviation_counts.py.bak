# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
from datetime import timedelta # Import timedelta
from dateutil.relativedelta import relativedelta
import sys

# --- Configuration ---
# !!! IMPORTANT: Update these constants to match the main calculation script and its output file !!!
# File with pre-calculated counts to validate (should be the output of the main calculation script)
INPUT_FILE_TO_VALIDATE = "participant_data_processed.csv" # Example: Output from calculate_test_deviations.py

# Calculation Parameters (Must exactly match the calculation script being validated)
GRACE_DAYS = 14 # Grace period in days (+/-)

# --- Column and Event Names (Must exactly match the calculation script) ---
# !! Update these variable values if your column names are different !!
ID_COL = "record_id"                  # Column containing the unique participant identifier
EVENT_NAME_COL = "event_name"           # Column containing the event/visit name
VISIT_DATE_COL = "visit_date"           # Column for the general visit date (used as baseline proxy)
TEST_DATE_COL = "test_date"             # Column for the specific date the test/assessment was performed
PARTICIPANT_STATUS_COL = "participant_status" # Column indicating participant status (e.g., active, withdrawn)
ENDPOINT_DATE_1_COL = "primary_endpoint_date"   # e.g., Date of event like disease flare, or end-of-study date for some statuses
ENDPOINT_DATE_2_COL = "secondary_endpoint_date" # e.g., Date of withdrawal or loss to follow-up for other statuses

# Count columns generated by the calculation script (to be validated)
MISSED_TEST_COUNT_COL = "missed_test_count"             # Count of expected tests with no date recorded
OUTSIDE_WINDOW_COUNT_COL = "outside_window_test_count"  # Count of performed tests outside the grace period
TOTAL_DEVIATION_COUNT_COL = "total_test_deviations"     # Sum of the above two counts

# Event Names (Must exactly match the calculation script)
BASELINE_EVENT_NAME = 'baseline_event_arm_1' # The specific name for the baseline/screening event
# Define the scheduled follow-up events and their expected month offset from the baseline event
# Format: {month_offset_from_baseline: 'event_name_in_data'}
EXPECTED_FOLLOWUP_SCHEDULE = {
      2: 'followup_visit_1',
      4: 'followup_visit_2',
      6: 'followup_visit_3',
      8: 'followup_visit_4',
     10: 'followup_visit_5',
     12: 'followup_visit_6',
     14: 'followup_visit_7',
     16: 'followup_visit_8',
     18: 'end_of_study_visit'
}
# List of all test event names, including baseline
ALL_TEST_EVENT_NAMES = [BASELINE_EVENT_NAME] + list(EXPECTED_FOLLOWUP_SCHEDULE.values())

# Status codes to validate calculation for
STATUSES_TO_RECALCULATE = [1, 2, 3, 4]
# Status code expected to have blank counts
STATUS_EXPECTING_BLANK = 5

print(f"--- Validating Calculated Deviation Counts in: {INPUT_FILE_TO_VALIDATE} ---")
print(f"Using Grace Period: +/- {GRACE_DAYS} days")
print(f"Using Proxy Baseline: '{VISIT_DATE_COL}' from baseline event (if '{TEST_DATE_COL}' is missing)")
print(f"Validating calculations for Statuses: {STATUSES_TO_RECALCULATE}")
print(f"Checking for blank counts for Status: {STATUS_EXPECTING_BLANK}")

# --- Load Data ---
print(f"\nLoading {INPUT_FILE_TO_VALIDATE}...")
try:
    # Attempt to read with default UTF-8 first, then fallback encodings
    try:
        df = pd.read_csv(INPUT_FILE_TO_VALIDATE, low_memory=False)
    except UnicodeDecodeError:
        try:
            df = pd.read_csv(INPUT_FILE_TO_VALIDATE, encoding='latin1', low_memory=False)
            print(" - Loaded using latin1 encoding.")
        except UnicodeDecodeError:
            df = pd.read_csv(INPUT_FILE_TO_VALIDATE, encoding='cp1252', low_memory=False)
            print(" - Loaded using cp1252 encoding.")

    print(f"Loaded {INPUT_FILE_TO_VALIDATE}. Shape: {df.shape}")
except FileNotFoundError:
    print(f"Error: File not found: {INPUT_FILE_TO_VALIDATE}")
    sys.exit(1)
except Exception as e:
    print(f"Error loading file: {e}")
    sys.exit(1)

# --- Data Cleaning & Preparation (Mirroring calculation script) ---
print("Preparing data for validation...")
# Clean column names
df.columns = df.columns.str.replace('\ufeff', '', regex=False).str.strip()
print(" - Cleaned column names.")

# Clean record_id column
if ID_COL in df.columns:
    df[ID_COL] = df[ID_COL].fillna('').astype(str).str.strip()
    print(f" - Cleaned '{ID_COL}' column.")
else:
     print(f"Error: Required column '{ID_COL}' not found.")
     sys.exit(1)

# Check essential columns exist
needed_cols_for_validation = {ID_COL, EVENT_NAME_COL, VISIT_DATE_COL, TEST_DATE_COL, PARTICIPANT_STATUS_COL,
                     ENDPOINT_DATE_1_COL, ENDPOINT_DATE_2_COL, MISSED_TEST_COUNT_COL,
                     OUTSIDE_WINDOW_COUNT_COL, TOTAL_DEVIATION_COUNT_COL}
missing_cols = needed_cols_for_validation - set(df.columns)
if missing_cols:
    print(f"Error: Input file missing critical columns for validation: {missing_cols}")
    sys.exit(1)

# Convert types (mirroring calculation script)
df[VISIT_DATE_COL] = pd.to_datetime(df[VISIT_DATE_COL], errors='coerce')
df[TEST_DATE_COL] = pd.to_datetime(df[TEST_DATE_COL], errors='coerce')
df[ENDPOINT_DATE_1_COL] = pd.to_datetime(df[ENDPOINT_DATE_1_COL], errors='coerce')
df[ENDPOINT_DATE_2_COL] = pd.to_datetime(df[ENDPOINT_DATE_2_COL], errors='coerce')
df[PARTICIPANT_STATUS_COL] = pd.to_numeric(df[PARTICIPANT_STATUS_COL], errors='coerce').astype('Int64')

# Convert STORED count columns to nullable integers for comparison
df[MISSED_TEST_COUNT_COL] = pd.to_numeric(df[MISSED_TEST_COUNT_COL], errors='coerce').astype('Int64')
df[OUTSIDE_WINDOW_COUNT_COL] = pd.to_numeric(df[OUTSIDE_WINDOW_COUNT_COL], errors='coerce').astype('Int64')
df[TOTAL_DEVIATION_COUNT_COL] = pd.to_numeric(df[TOTAL_DEVIATION_COUNT_COL], errors='coerce').astype('Int64')

# Ensure ID and Event columns are strings
df[ID_COL] = df[ID_COL].astype(str) # Already done, but ensure consistency
df[EVENT_NAME_COL] = df[EVENT_NAME_COL].astype(str).fillna('MISSING_EVENT')
print(" - Data types prepared.")


# --- Recalculation Logic (Mirrors calculation script EXACTLY) ---

# 1. Calculate Effective Baseline Dates
print("Recalculating baseline anchor dates...")
baseline_event_rows = df.loc[(df[EVENT_NAME_COL] == BASELINE_EVENT_NAME) & (df[ID_COL] != '')].copy()
baseline_event_rows = baseline_event_rows.sort_values([ID_COL]).drop_duplicates(ID_COL, keep='first')

baseline_event_rows['effective_baseline'] = pd.NaT
if TEST_DATE_COL in baseline_event_rows.columns and VISIT_DATE_COL in baseline_event_rows.columns:
    baseline_event_rows['effective_baseline'] = baseline_event_rows[TEST_DATE_COL].fillna(baseline_event_rows[VISIT_DATE_COL])
elif TEST_DATE_COL in baseline_event_rows.columns:
    baseline_event_rows['effective_baseline'] = baseline_event_rows[TEST_DATE_COL]
elif VISIT_DATE_COL in baseline_event_rows.columns:
    baseline_event_rows['effective_baseline'] = baseline_event_rows[VISIT_DATE_COL]
else: # Should not happen if columns were checked earlier
    print(f"Error: Missing date columns for baseline calculation during validation.")
    sys.exit(1)

baseline_event_rows["effective_baseline"] = pd.to_datetime(baseline_event_rows["effective_baseline"], errors='coerce')
final_baselines = baseline_event_rows.dropna(subset=["effective_baseline"])[[ID_COL, "effective_baseline"]].copy()
final_baselines[ID_COL] = final_baselines[ID_COL].astype(str)

# 2. Extract Participant Info from Baseline Rows
print("Extracting participant info from baseline rows...")
endpoint_cols_exist = [col for col in [ENDPOINT_DATE_1_COL, ENDPOINT_DATE_2_COL] if col in df.columns]
pinfo_raw = (
    df.loc[(df[ID_COL] != '') & (df[EVENT_NAME_COL] == BASELINE_EVENT_NAME),
           [ID_COL, PARTICIPANT_STATUS_COL] + endpoint_cols_exist]
    .drop_duplicates(subset=[ID_COL], keep='first')
    .copy()
)
pinfo_raw[ID_COL] = pinfo_raw[ID_COL].astype(str)

participants_for_recalc = pd.merge(pinfo_raw, final_baselines, on=ID_COL, how='inner')
participants_for_recalc = participants_for_recalc[participants_for_recalc[PARTICIPANT_STATUS_COL].isin(STATUSES_TO_RECALCULATE)].copy()
print(f"Recalculating counts for {len(participants_for_recalc)} participants (Statuses {STATUSES_TO_RECALCULATE} with baseline)...")

# 3. Create Performed Test Lookup Dictionary
performed_tests_df = df.loc[
    (df[EVENT_NAME_COL].isin(ALL_TEST_EVENT_NAMES)) &
    (df[TEST_DATE_COL].notna()) &
    (df[ID_COL] != '')
].copy()
performed_tests_df = performed_tests_df.sort_values([ID_COL, EVENT_NAME_COL, TEST_DATE_COL]).drop_duplicates(subset=[ID_COL, EVENT_NAME_COL], keep='first')
performed_dict = {(str(row[ID_COL]), row[EVENT_NAME_COL]): row[TEST_DATE_COL] for _, row in performed_tests_df.iterrows()}


# 4. Recalculate Counts Loop
recalc_results_list = []
if not participants_for_recalc.empty:
    for _, participant_row in participants_for_recalc.iterrows():
        pid = participant_row[ID_COL]
        base_dt = participant_row["effective_baseline"]
        status = participant_row[PARTICIPANT_STATUS_COL]

        # Handle potential NA status after filtering (shouldn't happen with isin but safety check)
        if pd.isna(status): continue
        status = int(status)

        endpoint1_dt = participant_row.get(ENDPOINT_DATE_1_COL, pd.NaT)
        endpoint2_dt = participant_row.get(ENDPOINT_DATE_2_COL, pd.NaT)

        recalc_missed = 0
        recalc_outside_window = 0

        # Determine endpoint date based on status (Exact mirror of calculation logic)
        participant_end_point_date = pd.NaT
        if status == 1:
            max_month_offset = max(EXPECTED_FOLLOWUP_SCHEDULE.keys()) if EXPECTED_FOLLOWUP_SCHEDULE else 0
            participant_end_point_date = base_dt + relativedelta(months=max_month_offset) # Adjusted to use max offset
        elif status == 2 and pd.notna(endpoint1_dt):
             participant_end_point_date = endpoint1_dt
        elif status in [3, 4] and pd.notna(endpoint2_dt): # Adjusted status check
             participant_end_point_date = endpoint2_dt

        if status in [2, 3, 4] and pd.isna(participant_end_point_date): continue # Skip if needed endpoint date missing
        if status == 1 and pd.isna(base_dt): continue # Skip if baseline missing for status 1

        # Check Baseline Test
        baseline_scheduled_date = base_dt
        baseline_is_expected = (status == 1) or (pd.notna(participant_end_point_date) and baseline_scheduled_date <= participant_end_point_date)

        if baseline_is_expected:
            baseline_performed_date = performed_dict.get((pid, BASELINE_EVENT_NAME))
            if pd.isna(baseline_performed_date):
                recalc_missed += 1
            else:
                baseline_lower = baseline_scheduled_date - timedelta(days=GRACE_DAYS)
                baseline_upper = baseline_scheduled_date + timedelta(days=GRACE_DAYS)
                if not (baseline_lower <= baseline_performed_date <= baseline_upper):
                    recalc_outside_window += 1

        # Check Follow-up Tests
        for month_offset, event_name in EXPECTED_FOLLOWUP_SCHEDULE.items():
            try:
                scheduled_date = base_dt + relativedelta(months=month_offset)
            except Exception: continue # Skip if date calculation fails

            # Check if this test is expected
            is_expected = False
            # Status 1 always expects up to the max defined offset if baseline exists
            if status == 1 and pd.notna(base_dt):
                 is_expected = True # Assumes all defined events are expected for Status 1
            # Other statuses expect based on endpoint date
            elif pd.notna(participant_end_point_date):
                # Test is expected if scheduled date is on or before the endpoint date
                is_expected = scheduled_date <= participant_end_point_date

            if is_expected:
                followup_key = (pid, event_name)
                performed_date = performed_dict.get(followup_key)

                if pd.isna(performed_date):
                    recalc_missed += 1
                else:
                    lower_bound = scheduled_date - timedelta(days=GRACE_DAYS)
                    upper_bound = scheduled_date + timedelta(days=GRACE_DAYS)
                    if not (lower_bound <= performed_date <= upper_bound):
                         recalc_outside_window += 1

        # Store recalculated values using the target column names for clarity
        recalc_results_list.append({
            ID_COL: pid,
            "recalc_" + MISSED_TEST_COUNT_COL: recalc_missed,
            "recalc_" + OUTSIDE_WINDOW_COUNT_COL: recalc_outside_window
        })

print(" - Recalculation complete.")

# --- Comparison (Status 1-4) ---
print(f"Comparing recalculated values with stored values for Statuses {STATUSES_TO_RECALCULATE}...")
if not recalc_results_list:
    print("Warning: No results were recalculated. Cannot perform comparison for Statuses {STATUSES_TO_RECALCULATE}.")
    comparison_df = pd.DataFrame() # Ensure comparison_df exists as empty
    num_checked = 0
else:
    recalc_df = pd.DataFrame(recalc_results_list)

    # Get stored values ONLY from the baseline row of the original DataFrame
    df_stored_counts = df.loc[(df[EVENT_NAME_COL] == BASELINE_EVENT_NAME) & (df[ID_COL] != '')].copy()
    # Ensure unique baseline row per ID if duplicates somehow exist
    df_stored_counts = df_stored_counts.sort_values([ID_COL]).drop_duplicates(ID_COL, keep='first')
    # Select only relevant columns
    df_stored_counts = df_stored_counts[[ID_COL, PARTICIPANT_STATUS_COL, MISSED_TEST_COUNT_COL, OUTSIDE_WINDOW_COUNT_COL, TOTAL_DEVIATION_COUNT_COL]].copy()

    # Merge stored values with recalculated values
    comparison_df = pd.merge(
        df_stored_counts,
        recalc_df,
        on=ID_COL,
        how='inner' # Only compare participants successfully recalculated AND having stored baseline counts
    )
    num_checked = len(comparison_df)
    print(f" - Comparing counts for {num_checked} participants.")

    # Perform direct comparison (using Int64 types from preparation)
    # Use helper function for robust NA comparison
    def compare_values(v1, v2):
         # Treat pandas NA and numpy NaN the same as missing
         v1_isna = pd.isna(v1)
         v2_isna = pd.isna(v2)
         if v1_isna and v2_isna: return True # Both missing is a match
         if v1_isna or v2_isna: return False # One missing, one not is a mismatch
         return v1 == v2 # Both have values, compare them

    comparison_df['missed_match'] = comparison_df.apply(lambda row: compare_values(row[MISSED_TEST_COUNT_COL], row["recalc_" + MISSED_TEST_COUNT_COL]), axis=1)
    comparison_df['outside_window_match'] = comparison_df.apply(lambda row: compare_values(row[OUTSIDE_WINDOW_COUNT_COL], row["recalc_" + OUTSIDE_WINDOW_COUNT_COL]), axis=1)

    # Recalculate total based on the RECALCULATED components
    comparison_df['recalc_' + TOTAL_DEVIATION_COUNT_COL] = comparison_df["recalc_" + MISSED_TEST_COUNT_COL].fillna(0) + comparison_df["recalc_" + OUTSIDE_WINDOW_COUNT_COL].fillna(0)
    comparison_df['total_match'] = comparison_df.apply(lambda row: compare_values(row[TOTAL_DEVIATION_COUNT_COL], row['recalc_' + TOTAL_DEVIATION_COUNT_COL]), axis=1)


# Find discrepancies for Status 1-4 (where any comparison is False)
discrepancies = comparison_df[
    ~(comparison_df['missed_match']) |
    ~(comparison_df['outside_window_match']) |
    ~(comparison_df['total_match'])
].copy() if not comparison_df.empty else pd.DataFrame() # Handle empty comparison_df case

num_discrepancies = len(discrepancies)

# --- Report Results (Status 1-4) ---
print(f"\n--- Validation Summary (Statuses {STATUSES_TO_RECALCULATE}) ---")
print(f"Total participants checked: {num_checked}")
print(f"Number of participants with discrepancies: {num_discrepancies}")

if num_discrepancies > 0:
    print(f"\nDetails of Discrepancies (Status {STATUSES_TO_RECALCULATE}):")
    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', 2000)

    # Display relevant columns: ID, Status, Stored, Recalculated
    cols_to_display = [
        ID_COL, PARTICIPANT_STATUS_COL,
        MISSED_TEST_COUNT_COL, "recalc_" + MISSED_TEST_COUNT_COL,
        OUTSIDE_WINDOW_COUNT_COL, "recalc_" + OUTSIDE_WINDOW_COUNT_COL,
        TOTAL_DEVIATION_COUNT_COL, 'recalc_' + TOTAL_DEVIATION_COUNT_COL
    ]
    # Ensure columns exist before trying to display
    cols_to_display_exist = [col for col in cols_to_display if col in discrepancies.columns]
    print(discrepancies[cols_to_display_exist].to_string(index=False))

    pd.reset_option('display.max_rows')
    pd.reset_option('display.max_columns')
    pd.reset_option('display.width')

    # Optional: Save discrepancies to CSV
    try:
        output_filename = "count_validation_discrepancies.csv"
        discrepancies[cols_to_display_exist].to_csv(output_filename, index=False, encoding='utf-8')
        print(f"\nDiscrepancy details saved to: {output_filename}")
    except Exception as e:
        print(f"\nError saving discrepancies to CSV: {e}", file=sys.stderr)

else:
    print(f"\n✅ No discrepancies found for Statuses {STATUSES_TO_RECALCULATE}. All recalculated values match stored values.")


# --- Check Status 5 Participants ---
print(f"\n--- Checking Status {STATUS_EXPECTING_BLANK} Participants ---")
# Find IDs with status 5 on their baseline row
status_5_baseline_ids = set()
if PARTICIPANT_STATUS_COL in df.columns:
    baseline_rows = df.loc[(df[EVENT_NAME_COL] == BASELINE_EVENT_NAME) & (df[ID_COL] != '')].copy()
    status_5_baseline_ids = set(baseline_rows.loc[baseline_rows[PARTICIPANT_STATUS_COL] == STATUS_EXPECTING_BLANK, ID_COL].astype(str).unique())
else:
    print(f"Warning: Status column '{PARTICIPANT_STATUS_COL}' not found. Cannot identify Status {STATUS_EXPECTING_BLANK} participants.")

print(f" - Found {len(status_5_baseline_ids)} unique participants with status {STATUS_EXPECTING_BLANK} on their baseline row.")

# Check ALL rows for these participants to ensure count columns are blank (NA)
status_5_errors = []
if status_5_baseline_ids:
    status_5_all_rows = df[df[ID_COL].isin(status_5_baseline_ids)].copy()
    cols_to_check_blank = [MISSED_TEST_COUNT_COL, OUTSIDE_WINDOW_COUNT_COL, TOTAL_DEVIATION_COUNT_COL]
    cols_exist_for_check = [col for col in cols_to_check_blank if col in status_5_all_rows.columns]

    if cols_exist_for_check:
         # Find rows where ANY of the count columns are NOT NA
         status_5_with_values = status_5_all_rows[status_5_all_rows[cols_exist_for_check].notna().any(axis=1)].copy()
         num_status_5_errors = len(status_5_with_values[ID_COL].unique()) # Count unique IDs with errors

         print(f"Number of Status {STATUS_EXPECTING_BLANK} participants with non-blank values in count columns (across ANY of their rows): {num_status_5_errors}")

         if num_status_5_errors > 0:
             print(f"\nDetails of Status {STATUS_EXPECTING_BLANK} Participants with Unexpected Values:")
             # Display the specific rows that have the non-blank values
             print(status_5_with_values[[ID_COL, EVENT_NAME_COL, PARTICIPANT_STATUS_COL] + cols_exist_for_check].to_string(index=False))
             # Save these specific error rows
             status_5_errors = status_5_with_values[[ID_COL, EVENT_NAME_COL, PARTICIPANT_STATUS_COL] + cols_exist_for_check].to_dict('records')
         else:
             print(f"✅ All Status {STATUS_EXPECTING_BLANK} participants correctly have blank values in count columns.")
    else:
         print(" - Count columns not found to check for Status {STATUS_EXPECTING_BLANK}.")
else:
    print(f" - No Status {STATUS_EXPECTING_BLANK} participants found to check.")

# Optional: Save Status 5 errors if any were found
if status_5_errors:
     try:
         output_filename_s5 = "status_5_unexpected_values.csv"
         pd.DataFrame(status_5_errors).to_csv(output_filename_s5, index=False, encoding='utf-8')
         print(f"\nStatus {STATUS_EXPECTING_BLANK} error details saved to: {output_filename_s5}")
     except Exception as e:
         print(f"\nError saving Status {STATUS_EXPECTING_BLANK} errors to CSV: {e}", file=sys.stderr)


print("\n--- Validation Script Finished ---")